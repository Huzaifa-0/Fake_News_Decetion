# -*- coding: utf-8 -*-
"""bert-fine-tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k6u_yTQTe11P0tLHY6mlkdaSUaQlEzdD
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)


real_df = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')
fake_df = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv') 

real_df.head()

fake_df.head()

real_df['label'] = 1
fake_df['label'] = 0
df = pd.concat([real_df, fake_df], axis=0, ignore_index=True)

# drop columns we aren't using in this application
data = df
data.drop(['title'], inplace=True, axis=1)
data.drop(['subject'], inplace=True, axis=1)
data.drop(['date'], inplace=True, axis=1)
data.head()

from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

# shuffle the data
df = shuffle(df).reset_index(drop=True)

# separate X and y
y = np.array(df.label)
X = df.drop(['label'], axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""Installing the required libraries:"""

!conda install -y pytorch torchvision cudatoolkit=10.1 -c pytorch
!pip install transformers

from transformers import BertTokenizer, BertForSequenceClassification
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")
model.config.num_labels = 1

"""I am going to freeze all the pretrained portion of the network and add a new section at the end of the network"""

# Freeze the pre trained parameters
for param in model.parameters():
    param.requires_grad = False

# Add three new layers at the end of the network
model.classifier = nn.Sequential(
    nn.Linear(768, 256),
    nn.ReLU(),
    nn.Linear(256, 64),
    nn.ReLU(),
    nn.Linear(64, 2),
    nn.Softmax(dim=1)
)

model = model.to(device)

criterion = nn.MSELoss().to(device)
optimizer = optim.SGD(model.classifier.parameters(), lr=0.01)

def preprocess_text(text):
    parts = []

    text_len = len(text.split(' '))
    delta = 300
    max_parts = 5
    nb_cuts = int(text_len / delta)
    nb_cuts = min(nb_cuts, max_parts)
    
    
    for i in range(nb_cuts + 1):
        text_part = ' '.join(text.split(' ')[i * delta: (i + 1) * delta])
        parts.append(tokenizer.encode(text_part, return_tensors="pt", max_length=500).to(device))

    return parts

print_every = 300

total_loss = 0
all_losses = []
y_train_iter = iter(y_train)
CUDA_LAUNCH_BLOCKING = 1
i = 0
model.train()

for idx, row in X_train.iterrows():
    text_parts = preprocess_text(str(row['text']))
    label = torch.tensor(next(y_train_iter)).long().to(device)

    optimizer.zero_grad()

    overall_output = torch.zeros((1, 2)).float().to(device)
    for part in text_parts:
        if len(part) > 0:
            try:
                input = part.reshape(-1)[:512].reshape(1, -1)
                overall_output += model(input, labels=label)[1].float().to(device)
            except Exception as e:
                print(str(e))

    overall_output = F.softmax(overall_output[0], dim=-1)

    if label == 0:
        label = torch.tensor([1.0, 0.0]).float().to(device)
    elif label == 1:
        label = torch.tensor([0.0, 1.0]).float().to(device)

    loss = criterion(overall_output, label)
    total_loss += loss.item()
    
    loss.backward()
    optimizer.step()

    if i % print_every == 0 and i > 0:
        average_loss = total_loss / print_every
        print("{}/{}. Average loss: {}".format(i, len(X_train), average_loss))
        all_losses.append(average_loss)
        total_loss = 0
    i += 1

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt

# %matplotlib inline
torch.save(model.state_dict(), "model_after_train.pt")

plt.plot(all_losses)

"""## Now I will test the accuracy of the model on the test set"""

total = len(X_test)
number_right = 0
i = 0
y_test_iter = iter(y_test)

y_preds = []
model.eval()

with torch.no_grad():
    for idx, row in X_test.iterrows():
        text_parts = preprocess_text(str(row['text']))
        label = torch.tensor(next(y_test_iter)).float().to(device)
        
        overall_output = torch.zeros((1,2)).to(device)
        try:
            for part in text_parts:
                if len(part) > 0:
                    overall_output += model(part.reshape(1, -1))[0]
        except RuntimeError:
            print("GPU out of memory, skipping this entry.")
            continue
            
        overall_output = F.softmax(overall_output[0], dim=-1)
            
        result = overall_output.max(0)[1].float().item()
 
        if result == label.item():
            number_right += 1
        y_preds.append(result)
            
        if i % print_every == 0 and i > 0:
            print("{}/{}. Current accuracy: {}".format(i, total, number_right / i))
        i += 1
print("Accuracy on test data: {}".format(number_right / total))

from sklearn.metrics import confusion_matrix
import seaborn as sns

confusion_matrix = confusion_matrix(y_pred=y_preds, y_true=y_test)

fig,ax = plt.subplots(figsize=(6,6))
sns.heatmap(confusion_matrix,annot=True,fmt="0.1f",linewidths=1.5)
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score
f1 = f1_score(y_true=y_test, y_pred=y_preds)
precision = precision_score(y_true=y_test, y_pred=y_preds)
recall = recall_score(y_true=y_test, y_pred=y_preds)
print(f'f1_score: {f1}')
print(f'precision: {precision}')
print(f'recall: {recall}')

